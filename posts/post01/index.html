<!doctype html><html><head><title>Web-scraping</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/logo5.png><link rel=stylesheet href=/assets/css/style.css><meta name=description content="Project 01 - Web scraping"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-122321624-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/logo3.png>Kiran Siddeshwar</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/logo3.png class=d-none id=main-logo>
<img src=/images/logo4.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><a class=active href=/posts/post01/>Web Scraping</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://kiransid0205.github.io/posts/post01/wallpaper.svg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/pic03.jpg><h5 class=author-name>Kiran Siddeshwar</h5><p>January 3, 2021</p></div><div class=title><h1>Web-scraping</h1></div><div class=post-content id=post-content><h5 id=i-what-is-web-scraping>I. What is Web scraping?</h5><p>Web scraping is the process of extracting/collecting useful data from the web in a structured manner. The scraping is usually done manually by a user, and the term refers to the automated process implemented using a web crawler/bot. Specific data is then gathered and copied into a spreadsheet/local database from the web.</p><p>Web scraping is used by people and businesses who want to make use of the vast amount of publicly available data online to make smarter decisions. Some of the main use cases of web scraping are:</p><ul><li>Online price change monitoring and price comparison.</li><li>Price intelligence.</li><li>News monitoring.</li><li>Lead generation.</li><li>Market research.</li><li>Web mining and data mining.</li><li>Product review scraping (to watch the competition).</li><li>Gathering real estate listings.</li><li>Weather data monitoring.</li><li>Website change detection.</li><li>Research.</li><li>Tracking online presence and reputation.</li></ul><p>The whole data scraping procedure consists of 2 main processes: Fetch and Extract. Fetching is the downloading of a page (which a browser does when a user views a page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then Extraction of data takes place. The content of a page may be parsed, searched, reformatted, and the data copied into a spreadsheet. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else.</p><p>Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form. However, most web pages are designed for human end-users and not for ease of automated use. As a result, specialized tools and software have been developed to facilitate the scraping of web pages.</p><h5 id=ii-ethics-of-web-scraping>II. Ethics of Web scraping</h5><p>Although web scraping has now become an increasingly common practice, there exists some baggage. The automated nature of scraping, along with its' power to make a difference always render it questionable. Hence, web scraping is often perceived as a shady, ‘black-hat’ practice; with a big question mark always hanging over the responsibility and accountability of the entities scraping the data.</p><p>Since web scraping is a simple yet powerful procedure, the need for legislation to control and regulate the procedure has been high. Under the EU’s General Data Protection Regulation (GDPR), &ldquo;Web scraping restrictions do not apply to a person or company unless such an entity extracts personal data of people within the European Economic Area&rdquo;. It is important to note that web scraping legislation varies by location and industry.</p><p>Best practices:</p><ol><li>Good web citizenship: Ethical web scraping begins with a commitment to good web citizenship. Familiarizing oneself with CFAA, GDPR, CAN-SPAM, REP, and other legislations would ensure there is no violation of any laws.</li><li>Don’t violate copyright: Whether or not it’s collected via web scraping, copyrighted information is off-limits without the written authorization of the copyright holder. Copyright protection comes into play once data is extracted. Copyright infringement is a quick and easy way to send an organization into a legal minefield, so tread carefully.</li><li>Limiting crawl rate and request frequency: Web scraping allows for quick automated tasks that would take users hours or days to complete. This makes web scraping great for quick gathering of data, but it can also cause problems by flooding sites with requests. Limiting the crawl rate and request frequency of web scraping projects allows to quickly gather data without causing problems on sites.</li><li>API usage: Many sites and products provide data access via an Automated Programming Interface. Depending on the type of data needed, APIs can be a good alternative to web scraping.</li><li>Terms of Service: The Terms of Service actually matter! Review a site’s ToS before scraping the data, ensuring nothing prohibitory is done.</li><li>Usage of public information: When an organization puts information on their website, they’re making it publicly available. This data is mostly non-sensitive data, hence it&rsquo;s legally allowed to scrape.</li></ol><h5 id=iii-tools-used>III. Tools used</h5><p>The web scraping in this project has been done using Python. Python has many useful packages and Beautiful Soup is one amongst them. It helps in data extraction by parsing HTML and XML documents; it creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.</p><p>Python 3 has been used for this task, and the bs4 version of the Beautiful Soup package has been applied for the data extraction.</p><p>Documentation of Beautiful Soup package can be found here - <a href=https://www.crummy.com/software/BeautifulSoup/bs4/doc/>https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></p><h5 id=iv-methodology>IV. Methodology</h5><ol><li>Identify the website to scrape data off of. In our case, it&rsquo;s daft.ie. We will filter our results to &ldquo;Houses for rent in Dublin&rdquo;.</li></ol><p><img src=./images/img01.png alt="&ldquo;Homepage of daft.ie&rdquo;">
<em>Fig 1.1 - Homepage of daft.ie</em></p><p><img src=./images/img02.png alt="&ldquo;Looking for houses on rent only in Dublin&rdquo;">
<em>Fig 1.2 - Filtering the data to show only accomodation for rent in Dublin</em></p><p><img src=./images/img03.png alt="&ldquo;A look at the data in daft.ie&rdquo;">
<em>Fig 1.3 - This is how the data in daft.ie looks like</em></p><ol start=2><li>The tool used is Python (version 3) on Jupyter Notebook.</li></ol><p><img src=./images/img04.png alt="&ldquo;Loading all necessary libraries&rdquo;">
<em>Fig 2.1 - Loading all the necessary libraries</em></p><p><img src=./images/img12.png alt="&ldquo;Setting user agent string&rdquo;">
<em>Fig 2.2 - Setting the user agent string, which is an identification string (like an ID card). All browsers have a unique ‘user agent string’ that they identify themselves with. This means that most websites may look a tiny bit different in Chrome, Firefox, Safari and other browsers. Specifying the &lsquo;user agent string&rsquo; helps in optimal visual and computational performance.</em></p><ol start=3><li>For data extraction, parsing is required first. For data to be parsed, the element IDs and other attributes are examined.</li></ol><p><img src=./images/img05.png alt="&ldquo;Data parsing&rdquo;">
<em>Fig 3.1 - Data parsing by checking the element</em></p><p><img src=./images/img06.png alt="&ldquo;Identification of elements&rdquo;">
<em>Fig 3.2 - Identification of elements which aid in data extraction</em></p><p><img src=./images/img10.png alt="&ldquo;Identification of elements&rdquo;">
<em>Fig 3.3 - Identification of elements which aid in data extraction</em></p><ol start=4><li>The scraping process is done using the bs4 library. Using the requests library, we get the desired URL with defined headers. After that, we create an object instance ‘soup’ that is used to find the necessary data on the page. The get() function gets access to data from the desired web page. BeautifulSoup() function creates a data structure representing a parsed HTML or XML document. The find_all() function extracts a list of Tag objects that match the given criteria. Any attributes of the Tag can be specified. All the parsed data is appended into nested lists.</li></ol><p><img src=./images/img07.PNG alt="&ldquo;Scraping of data&rdquo;">
<em>Fig 4.1 - Scraping of data using functions from BeautifulSoup</em></p><p><img src=./images/img08.PNG alt="&ldquo;Scraping of data&rdquo;">
<em>Fig 4.2 - Scraping of data using functions from BeautifulSoup</em></p><ol start=5><li>Each nested list is converted to a regular list for simplicity of usage.</li></ol><p><img src=./images/img09.PNG alt="&ldquo;Consolidation of data&rdquo;">
<em>Fig 5.1 - Consolidation of data into regular lists</em></p><ol start=6><li>It can be observed that some entries contain unnecessary information that do not contribute to the required objective, but contain the same tags as the house rental rates. These data end up extracted and have to be removed.</li></ol><p><img src=./images/img13.png alt="&ldquo;Extra data&rdquo;">
<em>Fig 6.1 - The extra unnecessary data</em></p><p><img src=./images/img11.PNG alt="&ldquo;Removal of extra data&rdquo;">
<em>Fig 6.2 - Removal of extra unnecessary data</em></p><ol start=7><li>The scraped data contains tags such as<p>and<div>. To remove this, the data is further filtered to the text and appended into lists.</li></ol><p><img src=./images/img14.PNG alt="&ldquo;Final filtering of data&rdquo;">
<em>Fig 7.1 - Removal of HTML tags from the scraped data</em></p><p><img src=./images/img15.PNG alt="&ldquo;Final filtering of data&rdquo;">
<em>Fig 7.2 - Removal of HTML tags from the scraped data</em></p><ol start=8><li>The data is now ready to be exported. This data requires further clean-up, which is easier to complete in MS Excel and then combined to one single dataset. This completes Part I of the web scraping process: Scraping of raw data from the web.</li></ol><p><img src=./images/img16.PNG alt="&ldquo;Exporting the data&rdquo;">
<em>Fig 8.1 - The filtered data is exported as .csv files</em></p><ol start=9><li><p>The exported data are: House-rent prices, bedrooms, bathrooms, housing-type, address and mixed data (this contains combination of bedrooms, bathrooms and housing-type). Some of the data of bedrooms, bathrooms and housing-type which were entered recently were combined (no idea why). But it is possible to separate them in a sensible manner.</p></li><li><p>The House-rent prices, bedrooms and bathrooms datasets are simple to clean up and extract. The REPLACE() function is used to remove unnecessary characters from the elements of the .csv file. The LEFT() function is used to extract the housing-type, bedrooms, bathrooms from the mixed data file.</p></li></ol><p><img src=./images/img21.png alt="&ldquo;Cleaning up data&rdquo;">
<em>Fig 10.1 - Extracting useful data from the raw scraped data</em></p><ol start=11><li>The extracted useful data is combined in a excel file and imported into Python again.</li></ol><p><img src=./images/img22.PNG alt="&ldquo;Clean data&rdquo;">
<em>Fig 11.1 - New cleaned-up data</em></p><p><img src=./images/img23.PNG alt="&ldquo;New dataset&rdquo;">
<em>Fig 11.2 - Importing the cleaned-up dataset</em></p><ol start=12><li>The address column contains useful data, but for the data to be used in a prediction model this data has to be filtered properly. The localities are identified and the respective pin codes are assigned to each house according to their locality.</li></ol><p><em>Fig 12.1 - Cleaning up the address column to extract pin codes</em>
<img src=./images/img24.PNG alt="&ldquo;Assigning pin code&rdquo;">
<img src=./images/img25.PNG alt="&ldquo;Assigning pin code&rdquo;">
<img src=./images/img26.PNG alt="&ldquo;Assigning pin code&rdquo;">
<img src=./images/img27.PNG alt="&ldquo;Assigning pin code&rdquo;"></p><ol start=13><li>Finally, the postal code list is added to the dataframe and exported as our final clean dataset, which can be used for our predictive model in the future. This completes Part II of the web scraping process.</li></ol><p><img src=./images/img28.PNG alt="&ldquo;Exporting the final clean data&rdquo;">
<em>Fig 13.1 - Exporting the final clean data</em></p><h5 id=v-conclusion>V. Conclusion</h5><p>Web-scraping is a useful and convenient way of collecting data for analysis. The use cases are innumerable and important. However, there is a potential of data privacy violation too, if not handled responsibly and properly. This activity has been a glimpse into what web-scraping can do and how it can be done. Also, this activity is a prequel to my next activity - House Rent prediction using Predictive Analytics.</p><p>Stay tuned!</p><p>Code uploaded on <a href=https://github.com/kiransid0205/My_projects/blob/main/web_scraping.ipynb>GitHub</a></p><p>Connect with me on <a href=https://www.linkedin.com/in/kiran-siddeshwar>LinkedIn</a>!</p><p>Thank you!</p><p>Kiran Siddeshwar</p><p><a href=https://kiransid0205.github.io/posts/post01/><img src=https://badges.pufler.dev/visits/kiransid0205.github.io/posts/post01 alt="Visits Badge"></a></p></div><div class=btn-improve-page><a href=https://github.com/kiransid0205/kiransid0205.github.io/edit/master/content/posts/post01/index.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/introduction/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Hi there!</span></a></div></div><hr></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><ul><li><ul><li><ul><li><a href=#i-what-is-web-scraping>I. What is Web scraping?</a></li><li><a href=#ii-ethics-of-web-scraping>II. Ethics of Web scraping</a></li><li><a href=#iii-tools-used>III. Tools used</a></li><li><a href=#iv-methodology>IV. Methodology</a></li><li><a href=#v-conclusion>V. Conclusion</a></li></ul></li></ul></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experience</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>krnsddswr@gmail.com</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>