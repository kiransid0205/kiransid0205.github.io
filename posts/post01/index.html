<!doctype html><html><head><title>Web-scraping</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/assets/css/bootstrap.min.css><link rel=stylesheet href=/assets/css/layouts/main.css><link rel=stylesheet href=/assets/css/style.css><link rel=stylesheet href=/assets/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/logo5.png><link rel=stylesheet href=/assets/css/style.css><meta name=description content="Project 01 - Web scraping"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/assets/css/layouts/single.css><link rel=stylesheet href=/assets/css/navigators/sidebar.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-122321624-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/logo3.png>Kiran Siddeshwar</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/logo3.png class=d-none id=main-logo>
<img src=/images/logo4.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><input type=text placeholder=Search data-search id=search-box><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><a class=active href=/posts/post01/>post01</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://kiransid0205.github.io/posts/introduction/hand.svg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/pic03.jpg><h5 class=author-name>Kiran Siddeshwar</h5><p>December 22, 2020</p></div><div class=title><h1>Web-scraping</h1></div><div class=post-content id=post-content><h5 id=i-what-is-web-scraping>I. What is Web scraping?</h5><p>Web scraping is the process of extracting/collecting useful data from the web in a structured manner. The scraping is usually done manually by a user, and the term refers to the automated process implemented using a web crawler/bot. Specific data is then gathered and copied into a spreadsheet/local database from the web.</p><p>Web scraping is used by people and businesses who want to make use of the vast amount of publicly available data online to make smarter decisions. Some of the main use cases of web scraping are:</p><ul><li>Online price change monitoring and price comparison.</li><li>Price intelligence.</li><li>News monitoring.</li><li>Lead generation.</li><li>Market research.</li><li>Web mining and data mining.</li><li>Product review scraping (to watch the competition).</li><li>Gathering real estate listings.</li><li>Weather data monitoring.</li><li>Website change detection.</li><li>Research.</li><li>Tracking online presence and reputation.</li></ul><p>The whole data scraping procedure consists of 2 main processes: Fetch and Extract. Fetching is the downloading of a page (which a browser does when a user views a page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then Extraction of data takes place. The content of a page may be parsed, searched, reformatted, and the data copied into a spreadsheet. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else.</p><p>Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form. However, most web pages are designed for human end-users and not for ease of automated use. As a result, specialized tools and software have been developed to facilitate the scraping of web pages.</p><h5 id=ii-ethics-of-web-scraping>II. Ethics of Web scraping</h5><p>Although web scraping has now become an increasingly common practice, there exists some baggage. The automated nature of scraping, along with its' power to make a difference always render it questionable. Hence, web scraping is often perceived as a shady, ‘black-hat’ practice; with a big question mark always hanging over the responsibility and accountability of the entities scraping the data.</p><p>Since it is a simple yet powerful procedure, the need for legislation to control and regulate web scraping was high. Under the EU’s General Data Protection Regulation (GDPR), &ldquo;Web scraping restrictions do not apply to a person or company unless such an entity extracts personal data of people within the European Economic Area&rdquo;. It is important to note that web scraping legislation varies by location and industry.</p><p>Best practices:</p><ol><li>Good web citizenship: Ethical web scraping begins with a commitment to good web citizenship. Familiarizing oneself with CFAA, GDPR, CAN-SPAM, REP, and other legislations would ensure there is no violation of any laws.</li><li>Don’t violate copyright: Whether or not it’s collected via web scraping, copyrighted information is off-limits without the written authorization of the copyright holder. Copyright protection comes into play once data is extracted. Copyright infringement is a quick and easy way to send an organization into a legal minefield, so tread carefully.</li><li>Limiting crawl rate and request frequency: Web scraping allows for quick automated tasks that would take users hours or days to complete. This makes web scraping great for quick gathering of data, but it can also cause problems by flooding sites with requests. Limiting the crawl rate and request frequency of web scraping projects allows to quickly gather data without causing problems on sites.</li><li>API usage: Many sites and products provide data access via an Automated Programming Interface. Depending on the type of data needed, APIs can be a good alternative to web scraping.</li><li>Terms of Service: The Terms of Service actually matter! Review a site’s ToS before scraping the data, ensuring nothing prohibitory is done.</li><li>Usage of public information: When an organization puts information on their website, they’re making it publicly available. This data is mostly non-sensitive data, hence it&rsquo;s legally allowed to scrape.</li></ol><h5 id=iii-tools-used>III. Tools used</h5><p>The web scraping in this project is being done using Python. Python has many useful packages and Beautiful Soup is one amongst them. It helps in data extraction by parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.</p><p>I have used Python 3 for this task, and have applied the bs4 version of the Beautiful Soup package for the data extraction.</p><p>Documentation of Beautiful Soup package: <a href=https://www.crummy.com/software/BeautifulSoup/bs4/doc/>https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></p><h5 id=iv-methodology>IV. Methodology</h5><ul><li>Identify the website to scrape data off of. In our case, it&rsquo;s daft.ie. We will filter our results only for houses for rent in Dublin.</li></ul><p><img src=./images/img01.png alt="&ldquo;Homepage of daft.ie&rdquo;">
<em>Homepage of daft.ie</em></p><p><img src=./images/img02.png alt="&ldquo;Looking for houses on rent only in Dublin&rdquo;">
<em>Filtering the data to Houses for rent in Dublin</em></p><p><img src=./images/img03.png alt="&ldquo;A look at the data in daft.ie&rdquo;">
<em>This is how the data in daft.ie looks like</em></p><ul><li>Next, the data extraction was done on Jupyter Notebook. Version of Python used: Python 3.</li></ul><p><img src=./images/img04.png alt="&ldquo;Loading all necessary libraries&rdquo;">
<em>Loading all necessary libraries</em></p><p><img src=./images/img12.png alt="&ldquo;Setting user agent string&rdquo;">
<em>Setting the user agent string, which is an identification string (like an ID card). All browsers have a unique ‘user agent string’ that they identify themselves with. This means that most websites may look a tiny bit different in Chrome, Firefox, Safari and other browsers. Specifying the &lsquo;user agent string&rsquo; helps in optimal visual and computational performance.</em></p><ul><li>For data extraction, parsing is required first. For data to be parsed, the element IDs and other attributes have to be examined.</li></ul><p><img src=./images/img05.png alt="&ldquo;Data parsing&rdquo;">
<em>Data parsing by checking the element</em></p><p><img src=./images/img06.png alt="&ldquo;Identification of elements&rdquo;">
<em>Identification of elements which aid in data extraction</em></p><p><img src=./images/img10.png alt="&ldquo;Identification of elements&rdquo;">
<em>Identification of elements which aid in data extraction</em></p><ul><li>The scraping process is done using the bs4 library. Using the requests library we got the desired URL with defined headers. After that, we created an object instance ‘soup’ that was used to find the necessary data on the page. The get() function gets access to data from the desired web page. BeautifulSoup() function creates a data structure representing a parsed HTML or XML document. The find_all() function extracts a list of Tag objects that match the given criteria. Any attributes of the Tag can be specified. All the parsed data is appended into nested lists.</li></ul><p><img src=./images/img07.png alt="&ldquo;Scraping of data&rdquo;">
<em>Scraping of data using functions from BeautifulSoup</em></p><p><img src=./images/img08.png alt="&ldquo;Scraping of data&rdquo;">
<em>Scraping of data using functions from BeautifulSoup</em></p><ul><li>Each nested list is then converted to a regular list for simplicity of usage.</li></ul><p><img src=./images/img09.png alt="&ldquo;Consolidation of data&rdquo;">
<em>Consolidation of data into regular lists</em></p><ul><li>It can be observed that some entries contain extra unnecessary information that do not contribute to the required objective, but contain the same tags as the house rental rates. These data end up extracted and have to be removed.</li></ul><p><img src=./images/img13.png alt="&ldquo;Extra data&rdquo;">
<em>The extra unnecessary data</em></p><p><img src=./images/img11.png alt="&ldquo;Removal of extra data&rdquo;">
<em>Removal of extra unnecessary data</em></p><ul><li>The scraped data contains tags such as<p>and<div>. To remove this, the data is further filtered to the text and appended into lists.</li></ul><p><img src=./images/img14.png alt="&ldquo;Final filtering of data&rdquo;">
<em>Removal of tags (like<p>and<div>) from the scraped data</em></p><p><img src=./images/img15.png alt="&ldquo;Final filtering of data&rdquo;">
<em>Removal of tags (like<p>and<div>) from the scraped data</em></p><ul><li>The data is now ready to be exported. This data required further clean-up, which feels easier to be completed in excel and then combined to one file. This completed Part I of the web scraping process: Scraping of raw data from the web.</li></ul><p><img src=./images/img16.png alt="&ldquo;Exporting the data&rdquo;">
<em>The filtered data is exported as .csv files</em></p><p>Code uploaded on <a href>GitHub</a></p><p>Connect with me on <a href=https://www.linkedin.com/in/kiran-siddeshwar>LinkedIn</a>!</p><p>Thank you!</p><p>Kiran Siddeshwar</p></div><div class=btn-improve-page><a href=https://github.com/kiransid0205/kiransid0205.github.io/edit/master/content/posts/post01/index.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/introduction/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Hi there!</span></a></div></div><hr></div></div></div></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><ul><li><ul><li><ul><li><a href=#i-what-is-web-scraping>I. What is Web scraping?</a></li><li><a href=#ii-ethics-of-web-scraping>II. Ethics of Web scraping</a></li><li><a href=#iii-tools-used>III. Tools used</a></li><li><a href=#iv-methodology>IV. Methodology</a></li></ul></li></ul></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experience</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>krnsddswr@gmail.com</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/assets/images/inverted-logo.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/assets/images/hugo-logo-wide.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/assets/js/jquery-3.4.1.min.js></script><script src=/assets/js/popper.min.js></script><script src=/assets/js/bootstrap.min.js></script><script src=/assets/js/navbar.js></script><script src=/assets/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/assets/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>